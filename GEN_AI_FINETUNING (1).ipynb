{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FV3U0l6oEwYw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aefde00b-5557-4938-a8bd-82d1c62dfb0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Project directory: /root/FinancialAI\n",
            "\n",
            "ğŸ’¡ Files will be saved to your local computer\n",
            "ğŸ’¡ Next: Set your OpenAI API key in Cell 2\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Initial Setup - Local Runtime\n",
        "\n",
        "import os\n",
        "\n",
        "# For local runtime - use your actual computer path\n",
        "# NOT Google Drive mounting\n",
        "\n",
        "# Create project directory on your local machine\n",
        "project_dir = os.path.expanduser('~/FinancialAI')  # This creates folder in your home directory\n",
        "os.makedirs(project_dir, exist_ok=True)\n",
        "\n",
        "print(f\"âœ… Project directory: {project_dir}\")\n",
        "print(\"\\nğŸ’¡ Files will be saved to your local computer\")\n",
        "print(\"ğŸ’¡ Next: Set your OpenAI API key in Cell 2\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Install packages and set API key (COMPLETE FIX)\n",
        "\n",
        "print(\"ğŸ“¦ Installing required packages...\")\n",
        "\n",
        "# Install specific compatible versions\n",
        "!pip install -q openai==1.3.0 datasets==2.14.0\n",
        "\n",
        "print(\"âœ… Packages installed!\")\n",
        "\n",
        "# Import and set API key\n",
        "import os\n",
        "\n",
        "# REPLACE WITH YOUR ACTUAL API KEY\n",
        "os.environ['OPENAI_API_KEY'] = ''\n",
        "\n",
        "print(\"\\nâœ… API key set!\")\n",
        "\n",
        "# Test API key with simple import (no client creation yet)\n",
        "try:\n",
        "    from openai import OpenAI\n",
        "    print(\"âœ… OpenAI library imported successfully!\")\n",
        "    print(\"ğŸ‰ Ready to proceed!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64QQG_58JnHS",
        "outputId": "88e9d53d-15aa-4902-ddc7-90ac6622b1b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“¦ Installing required packages...\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mâœ… Packages installed!\n",
            "\n",
            "âœ… API key set!\n",
            "âœ… OpenAI library imported successfully!\n",
            "ğŸ‰ Ready to proceed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2.5: Fix PyArrow compatibility issue\n",
        "\n",
        "print(\"ğŸ”§ Fixing PyArrow compatibility...\")\n",
        "\n",
        "# Uninstall and reinstall compatible versions\n",
        "!pip uninstall -y pyarrow datasets -q\n",
        "!pip install -q pyarrow==14.0.1 datasets==2.14.0\n",
        "\n",
        "print(\"âœ… Fixed!\")\n",
        "print(\"âš ï¸ IMPORTANT: Restart the kernel now!\")\n",
        "print(\"\\nSteps:\")\n",
        "print(\"1. Click 'Kernel' in the menu\")\n",
        "print(\"2. Click 'Restart Kernel'\")\n",
        "print(\"3. After restart, run Cell 1 and Cell 2 again\")\n",
        "print(\"4. Then run Cell 3\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCEzpXXKKO52",
        "outputId": "96a0144b-205f-47e6-856c-97d3a9e5af57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”§ Fixing PyArrow compatibility...\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mâœ… Fixed!\n",
            "âš ï¸ IMPORTANT: Restart the kernel now!\n",
            "\n",
            "Steps:\n",
            "1. Click 'Kernel' in the menu\n",
            "2. Click 'Restart Kernel'\n",
            "3. After restart, run Cell 1 and Cell 2 again\n",
            "4. Then run Cell 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Download FinQA Manually (Complete Solution)\n",
        "\n",
        "import requests\n",
        "import json\n",
        "import os\n",
        "\n",
        "print(\"ğŸ“š Downloading FinQA dataset manually...\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Project directory\n",
        "project_dir = os.path.expanduser('~/FinancialAI')\n",
        "\n",
        "# FinQA GitHub raw URLs\n",
        "urls = {\n",
        "    'train': 'https://raw.githubusercontent.com/czyssrs/FinQA/master/dataset/train.json',\n",
        "    'validation': 'https://raw.githubusercontent.com/czyssrs/FinQA/master/dataset/dev.json',\n",
        "    'test': 'https://raw.githubusercontent.com/czyssrs/FinQA/master/dataset/test.json'\n",
        "}\n",
        "\n",
        "dataset = {}\n",
        "\n",
        "for split, url in urls.items():\n",
        "    print(f\"\\nğŸ“¥ Downloading {split} data...\")\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, timeout=60)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        data = response.json()\n",
        "        dataset[split] = data\n",
        "\n",
        "        print(f\"âœ… {split}: {len(data):,} examples\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error downloading {split}: {e}\")\n",
        "        dataset[split] = []\n",
        "\n",
        "# Save locally\n",
        "for split, data in dataset.items():\n",
        "    filepath = os.path.join(project_dir, f'finqa_{split}.json')\n",
        "    with open(filepath, 'w') as f:\n",
        "        json.dump(data, f)\n",
        "    print(f\"ğŸ’¾ Saved {split} to: {filepath}\")\n",
        "\n",
        "print(f\"\\nâœ… Dataset downloaded successfully!\")\n",
        "print(f\"ğŸ“Š Training examples: {len(dataset['train']):,}\")\n",
        "print(f\"ğŸ“Š Validation examples: {len(dataset['validation']):,}\")\n",
        "print(f\"ğŸ“Š Test examples: {len(dataset['test']):,}\")\n",
        "\n",
        "# Inspect a sample\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ğŸ“„ SAMPLE EXAMPLE:\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "if len(dataset['train']) > 0:\n",
        "    sample = dataset['train'][0]\n",
        "\n",
        "    print(f\"\\nğŸ”¹ Pre-text: {sample.get('pre_text', 'N/A')[:200]}...\")\n",
        "    print(f\"\\nğŸ”¹ Question: {sample.get('question', 'N/A')}\")\n",
        "    print(f\"\\nğŸ”¹ Answer: {sample.get('answer', 'N/A')}\")\n",
        "else:\n",
        "    print(\"âš ï¸ No training data loaded\")\n",
        "\n",
        "print(\"\\nâœ… Dataset ready for formatting!\")"
      ],
      "metadata": {
        "id": "Ea6TZf8wFTLq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "049ab65c-17b6-407a-bb5a-e9256bc236c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“š Downloading FinQA dataset manually...\n",
            "======================================================================\n",
            "\n",
            "ğŸ“¥ Downloading train data...\n",
            "âœ… train: 6,251 examples\n",
            "\n",
            "ğŸ“¥ Downloading validation data...\n",
            "âœ… validation: 883 examples\n",
            "\n",
            "ğŸ“¥ Downloading test data...\n",
            "âœ… test: 1,147 examples\n",
            "ğŸ’¾ Saved train to: /root/FinancialAI/finqa_train.json\n",
            "ğŸ’¾ Saved validation to: /root/FinancialAI/finqa_validation.json\n",
            "ğŸ’¾ Saved test to: /root/FinancialAI/finqa_test.json\n",
            "\n",
            "âœ… Dataset downloaded successfully!\n",
            "ğŸ“Š Training examples: 6,251\n",
            "ğŸ“Š Validation examples: 883\n",
            "ğŸ“Š Test examples: 1,147\n",
            "\n",
            "======================================================================\n",
            "ğŸ“„ SAMPLE EXAMPLE:\n",
            "======================================================================\n",
            "\n",
            "ğŸ”¹ Pre-text: ['interest rate to a variable interest rate based on the three-month libor plus 2.05% ( 2.05 % ) ( 2.34% ( 2.34 % ) as of october 31 , 2009 ) .', 'if libor changes by 100 basis points , our annual interest expense would change by $ 3.8 million .', 'foreign currency exposure as more fully described in note 2i .', 'in the notes to consolidated financial statements contained in item 8 of this annual report on form 10-k , we regularly hedge our non-u.s .', 'dollar-based exposures by entering into forward foreign currency exchange contracts .', 'the terms of these contracts are for periods matching the duration of the underlying exposure and generally range from one month to twelve months .', 'currently , our largest foreign currency exposure is the euro , primarily because our european operations have the highest proportion of our local currency denominated expenses .', 'relative to foreign currency exposures existing at october 31 , 2009 and november 1 , 2008 , a 10% ( 10 % ) unfavorable movement in foreign currency exchange rates over the course of the year would not expose us to significant losses in earnings or cash flows because we hedge a high proportion of our year-end exposures against fluctuations in foreign currency exchange rates .', 'the market risk associated with our derivative instruments results from currency exchange rate or interest rate movements that are expected to offset the market risk of the underlying transactions , assets and liabilities being hedged .', 'the counterparties to the agreements relating to our foreign exchange instruments consist of a number of major international financial institutions with high credit ratings .', 'we do not believe that there is significant risk of nonperformance by these counterparties because we continually monitor the credit ratings of such counterparties .', 'while the contract or notional amounts of derivative financial instruments provide one measure of the volume of these transactions , they do not represent the amount of our exposure to credit risk .', 'the amounts potentially subject to credit risk ( arising from the possible inability of counterparties to meet the terms of their contracts ) are generally limited to the amounts , if any , by which the counterparties 2019 obligations under the contracts exceed our obligations to the counterparties .', 'the following table illustrates the effect that a 10% ( 10 % ) unfavorable or favorable movement in foreign currency exchange rates , relative to the u.s .', 'dollar , would have on the fair value of our forward exchange contracts as of october 31 , 2009 and november 1 , 2008: .']...\n",
            "\n",
            "ğŸ”¹ Question: N/A\n",
            "\n",
            "ğŸ”¹ Answer: N/A\n",
            "\n",
            "âœ… Dataset ready for formatting!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# COMPLETE FIX: Run this as Cell 4 (Replace old Cell 4)\n",
        "\n",
        "import json\n",
        "\n",
        "def is_safe_example(example):\n",
        "    \"\"\"Filter unsafe examples\"\"\"\n",
        "    # Access question and answer from the 'qa' sub-dictionary\n",
        "    qa = example.get('qa', {})\n",
        "    question = str(qa.get('question', '')).lower()\n",
        "    answer = str(qa.get('answer', '')).lower()\n",
        "\n",
        "    # Filter out examples with missing question or answer\n",
        "    if not question or question == 'n/a' or not answer or answer == 'n/a':\n",
        "        return False\n",
        "\n",
        "    # Filter unsafe keywords\n",
        "    unsafe = ['fraud', 'illegal', 'insider', 'should i invest', 'recommend buying']\n",
        "    if any(word in question + answer for word in unsafe):\n",
        "        return False\n",
        "\n",
        "    # Keep only factual questions (can be adjusted based on desired model behavior)\n",
        "    # For FinQA, most questions are factual, so a less strict filter might be fine.\n",
        "    # Let's be a bit more permissive, focusing on safety first.\n",
        "    # factual = ['what was', 'what is', 'how much', 'calculate', 'percentage']\n",
        "    # return any(word in question for word in factual)\n",
        "    return True # Allow all non-unsafe, non-empty questions for now\n",
        "\n",
        "\n",
        "def format_finqa_safe(example):\n",
        "    \"\"\"Format with safety\"\"\"\n",
        "\n",
        "    context = \"\"\n",
        "    if example.get('pre_text'):\n",
        "        context += '\\n'.join(example['pre_text']) + \"\\n\\n\"\n",
        "    if example.get('table'):\n",
        "        # Format table data more readably if it's a list of lists\n",
        "        table_str = \"\"\n",
        "        for row in example['table']:\n",
        "            table_str += '| ' + ' | '.join(map(str, row)) + ' |\\n'\n",
        "        context += \"Table:\\n\" + table_str + \"\\n\\n\"\n",
        "    if example.get('post_text'):\n",
        "        context += '\\n'.join(example['post_text'])\n",
        "\n",
        "    # Access question and answer from the 'qa' sub-dictionary\n",
        "    qa = example.get('qa', {})\n",
        "    question = str(qa.get('question', ''))\n",
        "    answer = str(qa.get('answer', ''))\n",
        "\n",
        "    if not question or question == 'N/A' or not answer or answer == 'N/A':\n",
        "        return None\n",
        "\n",
        "    return {\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are a financial analyst. Provide FACTUAL analysis only based on provided data. DO NOT give investment advice.\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"Context:\\n{context}\\n\\nQuestion: {question}\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": answer\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "\n",
        "\n",
        "# Filter and format\n",
        "print(\"ğŸ”„ Filtering and formatting safe data...\")\n",
        "\n",
        "training_data = []\n",
        "for example in dataset['train'][:1000]: # Limiting to 1000 examples for faster processing\n",
        "    if is_safe_example(example):\n",
        "        formatted = format_finqa_safe(example)\n",
        "        if formatted:\n",
        "            training_data.append(formatted)\n",
        "\n",
        "print(f\"âœ… Safe training examples: {len(training_data)}\")\n",
        "\n",
        "validation_data = []\n",
        "for example in dataset['validation'][:100]: # Limiting to 100 examples\n",
        "    if is_safe_example(example):\n",
        "        formatted = format_finqa_safe(example)\n",
        "        if formatted:\n",
        "            validation_data.append(formatted)\n",
        "\n",
        "print(f\"âœ… Safe validation examples: {len(validation_data)}\")"
      ],
      "metadata": {
        "id": "AtOK1NpmFTJC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b987c2a1-9103-4abf-d17a-6218a2fb0ba5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”„ Filtering and formatting safe data...\n",
            "âœ… Safe training examples: 984\n",
            "âœ… Safe validation examples: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Save formatted data as JSONL files (Local)\n",
        "\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Use local project directory\n",
        "project_dir = os.path.expanduser('~/FinancialAI')\n",
        "\n",
        "# File paths\n",
        "train_file = os.path.join(project_dir, 'finqa_train.jsonl')\n",
        "val_file = os.path.join(project_dir, 'finqa_validation.jsonl')\n",
        "\n",
        "print(\"ğŸ’¾ Saving formatted data...\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Save training data\n",
        "print(f\"\\nğŸ“ Saving training data to: {train_file}\")\n",
        "with open(train_file, 'w') as f:\n",
        "    for item in training_data:\n",
        "        f.write(json.dumps(item) + '\\n')\n",
        "\n",
        "print(f\"âœ… Saved {len(training_data)} training examples\")\n",
        "\n",
        "# Save validation data\n",
        "print(f\"\\nğŸ“ Saving validation data to: {val_file}\")\n",
        "with open(val_file, 'w') as f:\n",
        "    for item in validation_data:\n",
        "        f.write(json.dumps(item) + '\\n')\n",
        "\n",
        "print(f\"âœ… Saved {len(validation_data)} validation examples\")\n",
        "\n",
        "# Check file sizes\n",
        "train_size = os.path.getsize(train_file) / 1024 / 1024  # MB\n",
        "val_size = os.path.getsize(val_file) / 1024 / 1024\n",
        "\n",
        "print(f\"\\nğŸ“Š File sizes:\")\n",
        "print(f\"   Training: {train_size:.2f} MB\")\n",
        "print(f\"   Validation: {val_size:.2f} MB\")\n",
        "\n",
        "print(f\"\\nğŸ“ Files saved to: {project_dir}\")\n",
        "print(\"\\nâœ… Data saved successfully!\")"
      ],
      "metadata": {
        "id": "_XjGjgLkFTGS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbdf3039-d3db-4c4f-d9ec-4283ee60a756"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ Saving formatted data...\n",
            "======================================================================\n",
            "\n",
            "ğŸ“ Saving training data to: /root/FinancialAI/finqa_train.jsonl\n",
            "âœ… Saved 984 training examples\n",
            "\n",
            "ğŸ“ Saving validation data to: /root/FinancialAI/finqa_validation.jsonl\n",
            "âœ… Saved 100 validation examples\n",
            "\n",
            "ğŸ“Š File sizes:\n",
            "   Training: 4.09 MB\n",
            "   Validation: 0.42 MB\n",
            "\n",
            "ğŸ“ Files saved to: /root/FinancialAI\n",
            "\n",
            "âœ… Data saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2B: Fix OpenAI and httpx versions\n",
        "\n",
        "print(\"ğŸ”§ Fixing package versions...\")\n",
        "\n",
        "!pip uninstall -y openai httpx\n",
        "!pip install -q openai==1.3.0 httpx==0.24.1\n",
        "\n",
        "print(\"âœ… Fixed!\")\n",
        "print(\"âš ï¸ RESTART KERNEL NOW!\")\n",
        "print(\"\\nAfter restart:\")\n",
        "print(\"1. Run Cell 1 (setup)\")\n",
        "print(\"2. Run Cell 2 (API key)\")\n",
        "print(\"3. Continue from Cell 3\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9gvyG-jOMhu",
        "outputId": "b7286b6c-8f3e-4698-ce0f-dc2361e4a337"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”§ Fixing package versions...\n",
            "Found existing installation: openai 1.3.0\n",
            "Uninstalling openai-1.3.0:\n",
            "  Successfully uninstalled openai-1.3.0\n",
            "Found existing installation: httpx 0.24.1\n",
            "Uninstalling httpx-0.24.1:\n",
            "  Successfully uninstalled httpx-0.24.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mâœ… Fixed!\n",
            "âš ï¸ RESTART KERNEL NOW!\n",
            "\n",
            "After restart:\n",
            "1. Run Cell 1 (setup)\n",
            "2. Run Cell 2 (API key)\n",
            "3. Continue from Cell 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Upload to OpenAI (Error Handling Version)\n",
        "\n",
        "import os\n",
        "import json\n",
        "\n",
        "# Local project directory\n",
        "project_dir = os.path.expanduser('~/FinancialAI')\n",
        "train_file = os.path.join(project_dir, 'finqa_train.jsonl')\n",
        "val_file = os.path.join(project_dir, 'finqa_validation.jsonl')\n",
        "\n",
        "print(\"ğŸ“¤ Uploading training data to OpenAI...\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Try creating client with error handling\n",
        "try:\n",
        "    from openai import OpenAI\n",
        "    client = OpenAI(api_key=os.environ['OPENAI_API_KEY'])\n",
        "except TypeError:\n",
        "    # Fallback for version issues\n",
        "    print(\"âš ï¸ Using fallback client initialization...\")\n",
        "    import openai\n",
        "    openai.api_key = os.environ['OPENAI_API_KEY']\n",
        "    from openai import OpenAI\n",
        "    client = OpenAI()\n",
        "\n",
        "# Upload training file\n",
        "print(\"\\nğŸ“¤ Uploading training file...\")\n",
        "print(f\"   File: {train_file}\")\n",
        "\n",
        "with open(train_file, 'rb') as f:\n",
        "    train_upload = client.files.create(\n",
        "        file=f,\n",
        "        purpose='fine-tune'\n",
        "    )\n",
        "\n",
        "train_file_id = train_upload.id\n",
        "print(f\"âœ… Training file uploaded!\")\n",
        "print(f\"   File ID: {train_file_id}\")\n",
        "\n",
        "# Upload validation file\n",
        "print(\"\\nğŸ“¤ Uploading validation file...\")\n",
        "print(f\"   File: {val_file}\")\n",
        "\n",
        "with open(val_file, 'rb') as f:\n",
        "    val_upload = client.files.create(\n",
        "        file=f,\n",
        "        purpose='fine-tune'\n",
        "    )\n",
        "\n",
        "val_file_id = val_upload.id\n",
        "print(f\"âœ… Validation file uploaded!\")\n",
        "print(f\"   File ID: {val_file_id}\")\n",
        "\n",
        "# Save file IDs\n",
        "file_ids = {\n",
        "    'train_file_id': train_file_id,\n",
        "    'val_file_id': val_file_id\n",
        "}\n",
        "\n",
        "file_ids_path = os.path.join(project_dir, 'openai_file_ids.json')\n",
        "with open(file_ids_path, 'w') as f:\n",
        "    json.dump(file_ids, f, indent=2)\n",
        "\n",
        "print(f\"\\nğŸ’¾ File IDs saved to: {file_ids_path}\")\n",
        "print(\"\\nâœ… Upload complete! Ready for fine-tuning!\")"
      ],
      "metadata": {
        "id": "3RLc9ngcFTDy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "165472e1-2e09-4463-b9b3-a6b26220166e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“¤ Uploading training data to OpenAI...\n",
            "======================================================================\n",
            "\n",
            "ğŸ“¤ Uploading training file...\n",
            "   File: /root/FinancialAI/finqa_train.jsonl\n",
            "âœ… Training file uploaded!\n",
            "   File ID: file-AA35b78oGC4CYJrkLNv2uk\n",
            "\n",
            "ğŸ“¤ Uploading validation file...\n",
            "   File: /root/FinancialAI/finqa_validation.jsonl\n",
            "âœ… Validation file uploaded!\n",
            "   File ID: file-F8AhNTR4561dG47nBdAycq\n",
            "\n",
            "ğŸ’¾ File IDs saved to: /root/FinancialAI/openai_file_ids.json\n",
            "\n",
            "âœ… Upload complete! Ready for fine-tuning!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Fine-Tune GPT-4o-mini (Safer Model)\n",
        "\n",
        "from openai import OpenAI\n",
        "import json\n",
        "import os\n",
        "\n",
        "project_dir = os.path.expanduser('~/FinancialAI')\n",
        "\n",
        "# Load file IDs\n",
        "with open(os.path.join(project_dir, 'openai_file_ids.json'), 'r') as f:\n",
        "    file_ids = json.load(f)\n",
        "\n",
        "client = OpenAI(api_key=os.environ['OPENAI_API_KEY'])\n",
        "\n",
        "print(\"ğŸš€ Starting GPT-4o-mini Fine-Tuning...\")\n",
        "print(\"=\"*70)\n",
        "print(\"âš ï¸  This is MORE EXPENSIVE but SAFER\")\n",
        "print(\"ğŸ’° Expected cost: ~$50-60\")\n",
        "print()\n",
        "\n",
        "# Confirm before proceeding\n",
        "confirm = input(\"Type 'yes' to proceed with GPT-4o-mini fine-tuning: \")\n",
        "\n",
        "if confirm.lower() != 'yes':\n",
        "    print(\"âŒ Cancelled\")\n",
        "else:\n",
        "    # Create fine-tuning job with GPT-4o-mini\n",
        "    fine_tune_job = client.fine_tuning.jobs.create(\n",
        "        training_file=file_ids['train_file_id'],\n",
        "        validation_file=file_ids['val_file_id'],\n",
        "        model=\"gpt-4o-2024-08-06\",  # â† Newer, safer model\n",
        "        hyperparameters={\n",
        "            \"n_epochs\": 3\n",
        "        },\n",
        "        suffix=\"finqa-financial\"\n",
        "    )\n",
        "\n",
        "    job_id = fine_tune_job.id\n",
        "\n",
        "    print(f\"\\nâœ… Fine-tuning job started!\")\n",
        "    print(f\"ğŸ“‹ Job ID: {job_id}\")\n",
        "    print(f\"ğŸ“Š Model: gpt-4o-mini-2024-07-18\")\n",
        "\n",
        "    # Save job info\n",
        "    job_info = {\n",
        "        'job_id': job_id,\n",
        "        'model': 'gpt-4o-mini-2024-07-18',\n",
        "        'training_file': file_ids['train_file_id'],\n",
        "        'validation_file': file_ids['val_file_id'],\n",
        "        'n_epochs': 3,\n",
        "        'num_examples': len(training_data) if 'training_data' in dir() else 1000\n",
        "    }\n",
        "\n",
        "    with open(os.path.join(project_dir, 'finetuning_job_info.json'), 'w') as f:\n",
        "        json.dump(job_info, f, indent=2)\n",
        "\n",
        "    print(f\"\\nâ³ Run Cell 8 to monitor progress\")"
      ],
      "metadata": {
        "id": "SyHqAZKyFTBC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e7b6aa7-d85b-421c-8cf8-b65ea3892712"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ Starting GPT-4o-mini Fine-Tuning...\n",
            "======================================================================\n",
            "âš ï¸  This is MORE EXPENSIVE but SAFER\n",
            "ğŸ’° Expected cost: ~$50-60\n",
            "\n",
            "Type 'yes' to proceed with GPT-4o-mini fine-tuning: yes\n",
            "\n",
            "âœ… Fine-tuning job started!\n",
            "ğŸ“‹ Job ID: ftjob-B6muR1Rh2BEe5JWIs5W9O3Jv\n",
            "ğŸ“Š Model: gpt-4o-mini-2024-07-18\n",
            "\n",
            "â³ Run Cell 8 to monitor progress\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Monitor Fine-Tuning Status (Local Runtime)\n",
        "\n",
        "from openai import OpenAI\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "# Local project directory\n",
        "project_dir = os.path.expanduser('~/FinancialAI')\n",
        "\n",
        "# Load job ID\n",
        "with open(os.path.join(project_dir, 'finetuning_job_info.json'), 'r') as f:\n",
        "    job_info = json.load(f)\n",
        "\n",
        "job_id = job_info['job_id']\n",
        "\n",
        "# Create OpenAI client\n",
        "client = OpenAI(api_key=os.environ['OPENAI_API_KEY'])\n",
        "\n",
        "print(\"ğŸ”„ Monitoring Fine-Tuning Progress...\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Job ID: {job_id}\")\n",
        "print(f\"Started monitoring at: {datetime.now().strftime('%H:%M:%S')}\")\n",
        "print(\"\\nğŸ’¡ This will check every 5 minutes until complete\")\n",
        "print(\"ğŸ’¡ You can stop and restart this cell anytime\\n\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Monitor loop\n",
        "check_count = 0\n",
        "\n",
        "while True:\n",
        "    check_count += 1\n",
        "\n",
        "    # Get job status\n",
        "    job = client.fine_tuning.jobs.retrieve(job_id)\n",
        "    status = job.status\n",
        "\n",
        "    print(f\"\\n[Check #{check_count}] {datetime.now().strftime('%H:%M:%S')}\")\n",
        "    print(f\"Status: {status}\")\n",
        "\n",
        "    if status == \"succeeded\":\n",
        "        print(\"\\n\" + \"ğŸ‰\"*35)\n",
        "        print(\"FINE-TUNING COMPLETE!\")\n",
        "        print(\"ğŸ‰\"*35)\n",
        "\n",
        "        finetuned_model_id = job.fine_tuned_model\n",
        "        print(f\"\\nâœ… Fine-tuned Model ID: {finetuned_model_id}\")\n",
        "\n",
        "        # Save model ID\n",
        "        model_info = {\n",
        "            'model_id': finetuned_model_id,\n",
        "            'base_model': 'gpt-3.5-turbo',\n",
        "            'job_id': job_id,\n",
        "            'completed_at': datetime.now().isoformat(),\n",
        "            'training_examples': job_info['num_examples'],\n",
        "            'epochs': job_info['n_epochs']\n",
        "        }\n",
        "\n",
        "        model_path = os.path.join(project_dir, 'finetuned_model_info.json')\n",
        "        with open(model_path, 'w') as f:\n",
        "            json.dump(model_info, f, indent=2)\n",
        "\n",
        "        print(f\"\\nğŸ’¾ Model info saved to: {model_path}\")\n",
        "\n",
        "        # Also save just the model ID for easy access\n",
        "        with open(os.path.join(project_dir, 'finetuned_model_id.txt'), 'w') as f:\n",
        "            f.write(finetuned_model_id)\n",
        "\n",
        "        print(\"\\nâœ… Ready to use in Integration notebook!\")\n",
        "        print(f\"ğŸ““ Next: Create Notebook 3 for integration\")\n",
        "\n",
        "        break\n",
        "\n",
        "    elif status == \"failed\":\n",
        "        print(\"\\nâŒ Fine-tuning FAILED!\")\n",
        "        print(f\"Error: {job.error}\")\n",
        "        break\n",
        "\n",
        "    elif status == \"cancelled\":\n",
        "        print(\"\\nâš ï¸  Fine-tuning was cancelled\")\n",
        "        break\n",
        "\n",
        "    else:\n",
        "        # Still running\n",
        "        if hasattr(job, 'trained_tokens'):\n",
        "            print(f\"Progress: {job.trained_tokens} tokens trained\")\n",
        "\n",
        "        print(\"â³ Still training... will check again in 5 minutes\")\n",
        "        print(\"ğŸ’¤ Sleeping...\")\n",
        "\n",
        "        time.sleep(300)  # Wait 5 minutes\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Monitoring ended at:\", datetime.now().strftime('%H:%M:%S'))"
      ],
      "metadata": {
        "id": "fTS2Myt1FS-i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c46afc67-bb09-4522-8ce9-55f231045e53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”„ Monitoring Fine-Tuning Progress...\n",
            "======================================================================\n",
            "Job ID: ftjob-B6muR1Rh2BEe5JWIs5W9O3Jv\n",
            "Started monitoring at: 02:43:01\n",
            "\n",
            "ğŸ’¡ This will check every 5 minutes until complete\n",
            "ğŸ’¡ You can stop and restart this cell anytime\n",
            "\n",
            "======================================================================\n",
            "\n",
            "[Check #1] 02:43:01\n",
            "Status: validating_files\n",
            "Progress: None tokens trained\n",
            "â³ Still training... will check again in 5 minutes\n",
            "ğŸ’¤ Sleeping...\n",
            "\n",
            "[Check #2] 02:48:02\n",
            "Status: running\n",
            "Progress: None tokens trained\n",
            "â³ Still training... will check again in 5 minutes\n",
            "ğŸ’¤ Sleeping...\n",
            "\n",
            "[Check #3] 02:53:02\n",
            "Status: running\n",
            "Progress: None tokens trained\n",
            "â³ Still training... will check again in 5 minutes\n",
            "ğŸ’¤ Sleeping...\n",
            "\n",
            "[Check #4] 02:58:03\n",
            "Status: running\n",
            "Progress: None tokens trained\n",
            "â³ Still training... will check again in 5 minutes\n",
            "ğŸ’¤ Sleeping...\n",
            "\n",
            "[Check #5] 03:03:03\n",
            "Status: running\n",
            "Progress: None tokens trained\n",
            "â³ Still training... will check again in 5 minutes\n",
            "ğŸ’¤ Sleeping...\n",
            "\n",
            "[Check #6] 03:08:03\n",
            "Status: running\n",
            "Progress: None tokens trained\n",
            "â³ Still training... will check again in 5 minutes\n",
            "ğŸ’¤ Sleeping...\n",
            "\n",
            "[Check #7] 03:13:03\n",
            "Status: running\n",
            "Progress: None tokens trained\n",
            "â³ Still training... will check again in 5 minutes\n",
            "ğŸ’¤ Sleeping...\n",
            "\n",
            "[Check #8] 03:18:04\n",
            "Status: running\n",
            "Progress: None tokens trained\n",
            "â³ Still training... will check again in 5 minutes\n",
            "ğŸ’¤ Sleeping...\n",
            "\n",
            "[Check #9] 03:23:04\n",
            "Status: running\n",
            "Progress: None tokens trained\n",
            "â³ Still training... will check again in 5 minutes\n",
            "ğŸ’¤ Sleeping...\n",
            "\n",
            "[Check #10] 03:28:04\n",
            "Status: running\n",
            "Progress: None tokens trained\n",
            "â³ Still training... will check again in 5 minutes\n",
            "ğŸ’¤ Sleeping...\n",
            "\n",
            "[Check #11] 03:33:05\n",
            "Status: running\n",
            "Progress: None tokens trained\n",
            "â³ Still training... will check again in 5 minutes\n",
            "ğŸ’¤ Sleeping...\n",
            "\n",
            "[Check #12] 03:38:05\n",
            "Status: running\n",
            "Progress: None tokens trained\n",
            "â³ Still training... will check again in 5 minutes\n",
            "ğŸ’¤ Sleeping...\n",
            "\n",
            "[Check #13] 03:43:05\n",
            "Status: running\n",
            "Progress: None tokens trained\n",
            "â³ Still training... will check again in 5 minutes\n",
            "ğŸ’¤ Sleeping...\n",
            "\n",
            "[Check #14] 03:48:05\n",
            "Status: running\n",
            "Progress: None tokens trained\n",
            "â³ Still training... will check again in 5 minutes\n",
            "ğŸ’¤ Sleeping...\n",
            "\n",
            "[Check #15] 03:53:06\n",
            "Status: running\n",
            "Progress: None tokens trained\n",
            "â³ Still training... will check again in 5 minutes\n",
            "ğŸ’¤ Sleeping...\n",
            "\n",
            "[Check #16] 03:58:06\n",
            "Status: running\n",
            "Progress: None tokens trained\n",
            "â³ Still training... will check again in 5 minutes\n",
            "ğŸ’¤ Sleeping...\n",
            "\n",
            "[Check #17] 04:03:06\n",
            "Status: running\n",
            "Progress: None tokens trained\n",
            "â³ Still training... will check again in 5 minutes\n",
            "ğŸ’¤ Sleeping...\n",
            "\n",
            "[Check #18] 04:08:07\n",
            "Status: running\n",
            "Progress: None tokens trained\n",
            "â³ Still training... will check again in 5 minutes\n",
            "ğŸ’¤ Sleeping...\n",
            "\n",
            "[Check #19] 04:13:07\n",
            "Status: running\n",
            "Progress: None tokens trained\n",
            "â³ Still training... will check again in 5 minutes\n",
            "ğŸ’¤ Sleeping...\n",
            "\n",
            "[Check #20] 04:18:07\n",
            "Status: running\n",
            "Progress: None tokens trained\n",
            "â³ Still training... will check again in 5 minutes\n",
            "ğŸ’¤ Sleeping...\n",
            "\n",
            "[Check #21] 04:23:08\n",
            "Status: running\n",
            "Progress: None tokens trained\n",
            "â³ Still training... will check again in 5 minutes\n",
            "ğŸ’¤ Sleeping...\n",
            "\n",
            "[Check #22] 04:28:08\n",
            "Status: running\n",
            "Progress: None tokens trained\n",
            "â³ Still training... will check again in 5 minutes\n",
            "ğŸ’¤ Sleeping...\n",
            "\n",
            "[Check #23] 04:33:08\n",
            "Status: running\n",
            "Progress: None tokens trained\n",
            "â³ Still training... will check again in 5 minutes\n",
            "ğŸ’¤ Sleeping...\n",
            "\n",
            "[Check #24] 04:38:08\n",
            "Status: running\n",
            "Progress: None tokens trained\n",
            "â³ Still training... will check again in 5 minutes\n",
            "ğŸ’¤ Sleeping...\n",
            "\n",
            "[Check #25] 04:43:09\n",
            "Status: running\n",
            "Progress: None tokens trained\n",
            "â³ Still training... will check again in 5 minutes\n",
            "ğŸ’¤ Sleeping...\n",
            "\n",
            "[Check #26] 04:48:09\n",
            "Status: running\n",
            "Progress: None tokens trained\n",
            "â³ Still training... will check again in 5 minutes\n",
            "ğŸ’¤ Sleeping...\n",
            "\n",
            "[Check #27] 04:53:09\n",
            "Status: running\n",
            "Progress: None tokens trained\n",
            "â³ Still training... will check again in 5 minutes\n",
            "ğŸ’¤ Sleeping...\n",
            "\n",
            "[Check #28] 04:58:10\n",
            "Status: running\n",
            "Progress: None tokens trained\n",
            "â³ Still training... will check again in 5 minutes\n",
            "ğŸ’¤ Sleeping...\n",
            "\n",
            "[Check #29] 05:03:10\n",
            "Status: running\n",
            "Progress: None tokens trained\n",
            "â³ Still training... will check again in 5 minutes\n",
            "ğŸ’¤ Sleeping...\n",
            "\n",
            "[Check #30] 05:08:10\n",
            "Status: running\n",
            "Progress: None tokens trained\n",
            "â³ Still training... will check again in 5 minutes\n",
            "ğŸ’¤ Sleeping...\n",
            "\n",
            "[Check #31] 05:13:10\n",
            "Status: running\n",
            "Progress: None tokens trained\n",
            "â³ Still training... will check again in 5 minutes\n",
            "ğŸ’¤ Sleeping...\n",
            "\n",
            "[Check #32] 05:18:11\n",
            "Status: running\n",
            "Progress: None tokens trained\n",
            "â³ Still training... will check again in 5 minutes\n",
            "ğŸ’¤ Sleeping...\n",
            "\n",
            "[Check #33] 05:23:11\n",
            "Status: running\n",
            "Progress: None tokens trained\n",
            "â³ Still training... will check again in 5 minutes\n",
            "ğŸ’¤ Sleeping...\n",
            "\n",
            "[Check #34] 05:28:11\n",
            "Status: running\n",
            "Progress: None tokens trained\n",
            "â³ Still training... will check again in 5 minutes\n",
            "ğŸ’¤ Sleeping...\n",
            "\n",
            "[Check #35] 05:33:12\n",
            "Status: running\n",
            "Progress: None tokens trained\n",
            "â³ Still training... will check again in 5 minutes\n",
            "ğŸ’¤ Sleeping...\n",
            "\n",
            "[Check #36] 05:38:12\n",
            "Status: running\n",
            "Progress: None tokens trained\n",
            "â³ Still training... will check again in 5 minutes\n",
            "ğŸ’¤ Sleeping...\n",
            "\n",
            "[Check #37] 05:43:12\n",
            "Status: running\n",
            "Progress: None tokens trained\n",
            "â³ Still training... will check again in 5 minutes\n",
            "ğŸ’¤ Sleeping...\n",
            "\n",
            "[Check #38] 05:48:12\n",
            "Status: running\n",
            "Progress: None tokens trained\n",
            "â³ Still training... will check again in 5 minutes\n",
            "ğŸ’¤ Sleeping...\n",
            "\n",
            "[Check #39] 05:53:13\n",
            "Status: running\n",
            "Progress: None tokens trained\n",
            "â³ Still training... will check again in 5 minutes\n",
            "ğŸ’¤ Sleeping...\n",
            "\n",
            "[Check #40] 05:58:13\n",
            "Status: running\n",
            "Progress: None tokens trained\n",
            "â³ Still training... will check again in 5 minutes\n",
            "ğŸ’¤ Sleeping...\n",
            "\n",
            "[Check #41] 06:03:13\n",
            "Status: running\n",
            "Progress: None tokens trained\n",
            "â³ Still training... will check again in 5 minutes\n",
            "ğŸ’¤ Sleeping...\n",
            "\n",
            "[Check #42] 06:08:14\n",
            "Status: succeeded\n",
            "\n",
            "ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰\n",
            "FINE-TUNING COMPLETE!\n",
            "ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰\n",
            "\n",
            "âœ… Fine-tuned Model ID: ft:gpt-4o-2024-08-06:personal:finqa-financial:Chr7KFPi\n",
            "\n",
            "ğŸ’¾ Model info saved to: /root/FinancialAI/finetuned_model_info.json\n",
            "\n",
            "âœ… Ready to use in Integration notebook!\n",
            "ğŸ““ Next: Create Notebook 3 for integration\n",
            "\n",
            "======================================================================\n",
            "Monitoring ended at: 06:08:14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Test the Fine-Tuned Model (Local Runtime)\n",
        "\n",
        "from openai import OpenAI\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Local project directory\n",
        "project_dir = os.path.expanduser('~/FinancialAI')\n",
        "\n",
        "# Load fine-tuned model ID\n",
        "with open(os.path.join(project_dir, 'finetuned_model_id.txt'), 'r') as f:\n",
        "    finetuned_model_id = f.read().strip()\n",
        "\n",
        "# Create OpenAI client\n",
        "client = OpenAI(api_key=os.environ['OPENAI_API_KEY'])\n",
        "\n",
        "print(\"ğŸ§ª Testing Fine-Tuned Model\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Model: {finetuned_model_id}\\n\")\n",
        "\n",
        "# Test questions\n",
        "test_cases = [\n",
        "    {\n",
        "        \"context\": \"\"\"Apple Inc. Financial Data:\n",
        "\n",
        "Revenue breakdown for fiscal 2023:\n",
        "- iPhone: $200.6 billion\n",
        "- Mac: $29.4 billion\n",
        "- iPad: $28.3 billion\n",
        "- Wearables: $39.8 billion\n",
        "- Services: $85.2 billion\n",
        "\n",
        "Total net sales: $383.3 billion\n",
        "Cost of sales: $214.1 billion\n",
        "Gross profit: $169.2 billion\"\"\",\n",
        "        \"question\": \"What was Apple's gross margin percentage in fiscal 2023?\"\n",
        "    },\n",
        "    {\n",
        "        \"context\": \"\"\"Microsoft Corporation reported the following for fiscal year 2023:\n",
        "\n",
        "Segment Revenue:\n",
        "- Productivity and Business Processes: $69.3 billion\n",
        "- Intelligent Cloud: $87.9 billion\n",
        "- More Personal Computing: $54.7 billion\n",
        "\n",
        "Total Revenue: $211.9 billion\n",
        "Operating Income: $88.5 billion\"\"\",\n",
        "        \"question\": \"What percentage of Microsoft's total revenue came from Intelligent Cloud?\"\n",
        "    },\n",
        "    {\n",
        "        \"context\": \"\"\"Tesla Financial Summary:\n",
        "\n",
        "Q1 2023: Revenue $23.3B, Net Income $2.5B\n",
        "Q2 2023: Revenue $24.9B, Net Income $2.7B\n",
        "Q3 2023: Revenue $23.4B, Net Income $1.9B\n",
        "Q4 2023: Revenue $25.2B, Net Income $2.3B\"\"\",\n",
        "        \"question\": \"What was Tesla's total revenue for 2023?\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Test each case\n",
        "for i, test in enumerate(test_cases, 1):\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"TEST {i}/{len(test_cases)}\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    print(f\"\\nğŸ“„ Context:\\n{test['context']}\")\n",
        "    print(f\"\\nâ“ Question: {test['question']}\")\n",
        "\n",
        "    # Get answer from fine-tuned model\n",
        "    response = client.chat.completions.create(\n",
        "        model=finetuned_model_id,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are an expert financial analyst. Answer questions accurately based on the provided financial data.\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"Context:\\n{test['context']}\\n\\nQuestion: {test['question']}\"\n",
        "            }\n",
        "        ],\n",
        "        temperature=0.3,\n",
        "        max_tokens=300\n",
        "    )\n",
        "\n",
        "    answer = response.choices[0].message.content\n",
        "\n",
        "    print(f\"\\nğŸ’¡ Fine-Tuned Model Answer:\")\n",
        "    print(f\"{answer}\")\n",
        "\n",
        "    print(f\"\\nâœ… Test {i} complete\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ğŸ‰ ALL TESTS COMPLETE!\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nâœ… Fine-tuned model is working correctly!\")\n",
        "print(f\"ğŸ““ Model ID: {finetuned_model_id}\")\n",
        "print(\"\\nğŸš€ Next Step: Create Notebook 3 for full integration\")"
      ],
      "metadata": {
        "id": "-ACNsfbxFS8C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "514affa0-9443-4f86-d3e0-5815c3946089"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ§ª Testing Fine-Tuned Model\n",
            "======================================================================\n",
            "Model: ft:gpt-4o-2024-08-06:personal:finqa-financial:Chr7KFPi\n",
            "\n",
            "\n",
            "======================================================================\n",
            "TEST 1/3\n",
            "======================================================================\n",
            "\n",
            "ğŸ“„ Context:\n",
            "Apple Inc. Financial Data:\n",
            "        \n",
            "Revenue breakdown for fiscal 2023:\n",
            "- iPhone: $200.6 billion\n",
            "- Mac: $29.4 billion  \n",
            "- iPad: $28.3 billion\n",
            "- Wearables: $39.8 billion\n",
            "- Services: $85.2 billion\n",
            "\n",
            "Total net sales: $383.3 billion\n",
            "Cost of sales: $214.1 billion\n",
            "Gross profit: $169.2 billion\n",
            "\n",
            "â“ Question: What was Apple's gross margin percentage in fiscal 2023?\n",
            "\n",
            "ğŸ’¡ Fine-Tuned Model Answer:\n",
            "44.1%\n",
            "\n",
            "âœ… Test 1 complete\n",
            "\n",
            "======================================================================\n",
            "TEST 2/3\n",
            "======================================================================\n",
            "\n",
            "ğŸ“„ Context:\n",
            "Microsoft Corporation reported the following for fiscal year 2023:\n",
            "\n",
            "Segment Revenue:\n",
            "- Productivity and Business Processes: $69.3 billion\n",
            "- Intelligent Cloud: $87.9 billion  \n",
            "- More Personal Computing: $54.7 billion\n",
            "\n",
            "Total Revenue: $211.9 billion\n",
            "Operating Income: $88.5 billion\n",
            "\n",
            "â“ Question: What percentage of Microsoft's total revenue came from Intelligent Cloud?\n",
            "\n",
            "ğŸ’¡ Fine-Tuned Model Answer:\n",
            "41%\n",
            "\n",
            "âœ… Test 2 complete\n",
            "\n",
            "======================================================================\n",
            "TEST 3/3\n",
            "======================================================================\n",
            "\n",
            "ğŸ“„ Context:\n",
            "Tesla Financial Summary:\n",
            "\n",
            "Q1 2023: Revenue $23.3B, Net Income $2.5B\n",
            "Q2 2023: Revenue $24.9B, Net Income $2.7B\n",
            "Q3 2023: Revenue $23.4B, Net Income $1.9B  \n",
            "Q4 2023: Revenue $25.2B, Net Income $2.3B\n",
            "\n",
            "â“ Question: What was Tesla's total revenue for 2023?\n",
            "\n",
            "ğŸ’¡ Fine-Tuned Model Answer:\n",
            "96.8\n",
            "\n",
            "âœ… Test 3 complete\n",
            "\n",
            "======================================================================\n",
            "ğŸ‰ ALL TESTS COMPLETE!\n",
            "======================================================================\n",
            "\n",
            "âœ… Fine-tuned model is working correctly!\n",
            "ğŸ““ Model ID: ft:gpt-4o-2024-08-06:personal:finqa-financial:Chr7KFPi\n",
            "\n",
            "ğŸš€ Next Step: Create Notebook 3 for full integration\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10: Estimate Fine-Tuning Cost (Local Runtime)\n",
        "\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Local project directory\n",
        "project_dir = os.path.expanduser('~/FinancialAI')\n",
        "\n",
        "# Load job info\n",
        "with open(os.path.join(project_dir, 'finetuning_job_info.json'), 'r') as f:\n",
        "    job_info = json.load(f)\n",
        "\n",
        "num_examples = job_info['num_examples']\n",
        "n_epochs = job_info['n_epochs']\n",
        "\n",
        "# Estimate tokens per example (average for FinQA)\n",
        "avg_tokens_per_example = 800  # Context + Question + Answer\n",
        "\n",
        "# Total training tokens\n",
        "total_tokens = num_examples * n_epochs * avg_tokens_per_example\n",
        "\n",
        "# OpenAI pricing (as of Nov 2024)\n",
        "training_cost_per_1k_tokens = 0.008  # $0.008 per 1K tokens\n",
        "\n",
        "# Calculate cost\n",
        "estimated_cost = (total_tokens / 1000) * training_cost_per_1k_tokens\n",
        "\n",
        "print(\"ğŸ’° FINE-TUNING COST ESTIMATION\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nğŸ“Š Training Details:\")\n",
        "print(f\"   Examples: {num_examples:,}\")\n",
        "print(f\"   Epochs: {n_epochs}\")\n",
        "print(f\"   Avg tokens/example: {avg_tokens_per_example}\")\n",
        "print(f\"\\nğŸ“Š Total Training Tokens: {total_tokens:,}\")\n",
        "print(f\"\\nğŸ’µ Estimated Cost: ${estimated_cost:.2f}\")\n",
        "\n",
        "print(f\"\\nğŸ’¡ Additional costs:\")\n",
        "print(f\"   â€¢ Inference: ~$0.002 per query\")\n",
        "print(f\"   â€¢ Total for 1000 queries: ~$2.00\")\n",
        "\n",
        "print(f\"\\nğŸ’° Total Project Cost Estimate: ${estimated_cost + 2:.2f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)"
      ],
      "metadata": {
        "id": "1lBYGftFFS5S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d0abbfb-fd9a-4701-fbae-337a9063238e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’° FINE-TUNING COST ESTIMATION\n",
            "======================================================================\n",
            "\n",
            "ğŸ“Š Training Details:\n",
            "   Examples: 984\n",
            "   Epochs: 3\n",
            "   Avg tokens/example: 800\n",
            "\n",
            "ğŸ“Š Total Training Tokens: 2,361,600\n",
            "\n",
            "ğŸ’µ Estimated Cost: $18.89\n",
            "\n",
            "ğŸ’¡ Additional costs:\n",
            "   â€¢ Inference: ~$0.002 per query\n",
            "   â€¢ Total for 1000 queries: ~$2.00\n",
            "\n",
            "ğŸ’° Total Project Cost Estimate: $20.89\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this to get your model ID\n",
        "\n",
        "from openai import OpenAI\n",
        "import os\n",
        "import json\n",
        "\n",
        "project_dir = os.path.expanduser('~/FinancialAI')\n",
        "client = OpenAI(api_key=os.environ['OPENAI_API_KEY'])\n",
        "\n",
        "# Load job info\n",
        "with open(os.path.join(project_dir, 'finetuning_job_info.json'), 'r') as f:\n",
        "    job_info = json.load(f)\n",
        "\n",
        "job_id = job_info['job_id']\n",
        "\n",
        "# Get job details\n",
        "job = client.fine_tuning.jobs.retrieve(job_id)\n",
        "\n",
        "print(\"ğŸ‰ FINE-TUNING COMPLETE!\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Status: {job.status}\")\n",
        "print(f\"Model ID: {job.fine_tuned_model}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Save model ID\n",
        "finetuned_model_id = job.fine_tuned_model\n",
        "\n",
        "with open(os.path.join(project_dir, 'finetuned_model_id.txt'), 'w') as f:\n",
        "    f.write(finetuned_model_id)\n",
        "\n",
        "print(f\"\\nâœ… Model ID saved to: {project_dir}/finetuned_model_id.txt\")\n",
        "print(f\"\\nğŸ“‹ Your Fine-Tuned Model: {finetuned_model_id}\")"
      ],
      "metadata": {
        "id": "OoTo5fOsFS2i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e6c18f0-a487-40d5-e5d6-5e278be70bad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ‰ FINE-TUNING COMPLETE!\n",
            "======================================================================\n",
            "Status: succeeded\n",
            "Model ID: ft:gpt-4o-2024-08-06:personal:finqa-financial:Chr7KFPi\n",
            "======================================================================\n",
            "\n",
            "âœ… Model ID saved to: /root/FinancialAI/finetuned_model_id.txt\n",
            "\n",
            "ğŸ“‹ Your Fine-Tuned Model: ft:gpt-4o-2024-08-06:personal:finqa-financial:Chr7KFPi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell: Test Fine-Tuned GPT-4o Model\n",
        "\n",
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "project_dir = os.path.expanduser('~/FinancialAI')\n",
        "client = OpenAI(api_key=os.environ['OPENAI_API_KEY'])\n",
        "\n",
        "# Load model ID\n",
        "with open(os.path.join(project_dir, 'finetuned_model_id.txt'), 'r') as f:\n",
        "    finetuned_model_id = f.read().strip()\n",
        "\n",
        "print(\"ğŸ§ª Testing Fine-Tuned Model\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Model: {finetuned_model_id}\\n\")\n",
        "\n",
        "# Test with a financial question\n",
        "test_context = \"\"\"\n",
        "Apple Inc. Financial Data (Fiscal 2023):\n",
        "\n",
        "Revenue by Segment:\n",
        "- iPhone: $200.6 billion\n",
        "- Mac: $29.4 billion\n",
        "- iPad: $28.3 billion\n",
        "- Wearables, Home, Accessories: $39.8 billion\n",
        "- Services: $85.2 billion\n",
        "\n",
        "Total Revenue: $383.3 billion\n",
        "Cost of Revenue: $214.1 billion\n",
        "Gross Profit: $169.2 billion\n",
        "Operating Income: $114.3 billion\n",
        "Net Income: $97.0 billion\n",
        "\"\"\"\n",
        "\n",
        "test_question = \"What was Apple's gross profit margin in fiscal 2023?\"\n",
        "\n",
        "print(\"ğŸ“„ Context:\")\n",
        "print(test_context)\n",
        "print(f\"\\nâ“ Question: {test_question}\")\n",
        "\n",
        "# Call fine-tuned model\n",
        "response = client.chat.completions.create(\n",
        "    model=finetuned_model_id,  # â† Your fine-tuned model\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are an expert financial analyst. Answer questions accurately based on the provided financial data.\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"Context:\\n{test_context}\\n\\nQuestion: {test_question}\"\n",
        "        }\n",
        "    ],\n",
        "    temperature=0.3,\n",
        "    max_tokens=300\n",
        ")\n",
        "\n",
        "answer = response.choices[0].message.content\n",
        "\n",
        "print(\"\\nğŸ’¡ Fine-Tuned Model Answer:\")\n",
        "print(\"=\"*70)\n",
        "print(answer)\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nâœ… Fine-tuned model is working!\")"
      ],
      "metadata": {
        "id": "BxF3oESkFS0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "408f013c-4ce1-40ae-ef80-b1d4b815e16c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ§ª Testing Fine-Tuned Model\n",
            "======================================================================\n",
            "Model: ft:gpt-4o-2024-08-06:personal:finqa-financial:Chr7KFPi\n",
            "\n",
            "ğŸ“„ Context:\n",
            "\n",
            "Apple Inc. Financial Data (Fiscal 2023):\n",
            "\n",
            "Revenue by Segment:\n",
            "- iPhone: $200.6 billion\n",
            "- Mac: $29.4 billion\n",
            "- iPad: $28.3 billion\n",
            "- Wearables, Home, Accessories: $39.8 billion\n",
            "- Services: $85.2 billion\n",
            "\n",
            "Total Revenue: $383.3 billion\n",
            "Cost of Revenue: $214.1 billion\n",
            "Gross Profit: $169.2 billion\n",
            "Operating Income: $114.3 billion\n",
            "Net Income: $97.0 billion\n",
            "\n",
            "\n",
            "â“ Question: What was Apple's gross profit margin in fiscal 2023?\n",
            "\n",
            "ğŸ’¡ Fine-Tuned Model Answer:\n",
            "======================================================================\n",
            "44.1%\n",
            "======================================================================\n",
            "\n",
            "âœ… Fine-tuned model is working!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell: Compare Fine-Tuned vs Baseline\n",
        "\n",
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "client = OpenAI(api_key=os.environ['OPENAI_API_KEY'])\n",
        "project_dir = os.path.expanduser('~/FinancialAI')\n",
        "\n",
        "# Load fine-tuned model ID\n",
        "with open(os.path.join(project_dir, 'finetuned_model_id.txt'), 'r') as f:\n",
        "    finetuned_model_id = f.read().strip()\n",
        "\n",
        "# Test questions\n",
        "test_cases = [\n",
        "    {\n",
        "        \"context\": \"Company X: Revenue $100M (2022), $120M (2023). Operating Income $20M (2022), $28M (2023).\",\n",
        "        \"question\": \"What was the operating margin improvement?\"\n",
        "    },\n",
        "    {\n",
        "        \"context\": \"Tesla Q3 2023: Total revenue $23.4B, Automotive revenue $19.6B, Energy revenue $1.6B, Services $2.2B.\",\n",
        "        \"question\": \"What percentage of revenue came from automotive sales?\"\n",
        "    }\n",
        "]\n",
        "\n",
        "print(\"ğŸ”¬ COMPARISON: Fine-Tuned vs Baseline\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for i, test in enumerate(test_cases, 1):\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"TEST CASE {i}\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"\\nğŸ“„ Context: {test['context']}\")\n",
        "    print(f\"\\nâ“ Question: {test['question']}\")\n",
        "\n",
        "    prompt = f\"Context:\\n{test['context']}\\n\\nQuestion: {test['question']}\"\n",
        "\n",
        "    # Baseline GPT-3.5\n",
        "    print(\"\\nğŸ”µ BASELINE (GPT-3.5-turbo):\")\n",
        "    print(\"-\"*70)\n",
        "    baseline_response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a financial analyst.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=0.3\n",
        "    )\n",
        "    print(baseline_response.choices[0].message.content)\n",
        "\n",
        "    # Fine-tuned GPT-4o\n",
        "    print(\"\\nğŸŸ¢ FINE-TUNED (GPT-4o):\")\n",
        "    print(\"-\"*70)\n",
        "    finetuned_response = client.chat.completions.create(\n",
        "        model=finetuned_model_id,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a financial analyst.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=0.3\n",
        "    )\n",
        "    print(finetuned_response.choices[0].message.content)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"âœ… Comparison complete!\")"
      ],
      "metadata": {
        "id": "1k4i1JwqFSxa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb0c36ec-6217-4a5a-a4c0-3bd30b7c9d2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”¬ COMPARISON: Fine-Tuned vs Baseline\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "TEST CASE 1\n",
            "======================================================================\n",
            "\n",
            "ğŸ“„ Context: Company X: Revenue $100M (2022), $120M (2023). Operating Income $20M (2022), $28M (2023).\n",
            "\n",
            "â“ Question: What was the operating margin improvement?\n",
            "\n",
            "ğŸ”µ BASELINE (GPT-3.5-turbo):\n",
            "----------------------------------------------------------------------\n",
            "To calculate the operating margin improvement, we first need to calculate the operating margin for each year.\n",
            "\n",
            "Operating Margin = Operating Income / Revenue\n",
            "\n",
            "For 2022:\n",
            "Operating Margin 2022 = $20M / $100M = 0.20 or 20%\n",
            "\n",
            "For 2023:\n",
            "Operating Margin 2023 = $28M / $120M = 0.2333 or 23.33%\n",
            "\n",
            "Now, we can calculate the operating margin improvement by comparing the operating margin in 2023 to that in 2022:\n",
            "\n",
            "Operating Margin Improvement = Operating Margin 2023 - Operating Margin 2022\n",
            "Operating Margin Improvement = 23.33% - 20% = 3.33%\n",
            "\n",
            "Therefore, the operating margin improvement from 2022 to 2023 for Company X is 3.33%.\n",
            "\n",
            "ğŸŸ¢ FINE-TUNED (GPT-4o):\n",
            "----------------------------------------------------------------------\n",
            "2%\n",
            "\n",
            "======================================================================\n",
            "TEST CASE 2\n",
            "======================================================================\n",
            "\n",
            "ğŸ“„ Context: Tesla Q3 2023: Total revenue $23.4B, Automotive revenue $19.6B, Energy revenue $1.6B, Services $2.2B.\n",
            "\n",
            "â“ Question: What percentage of revenue came from automotive sales?\n",
            "\n",
            "ğŸ”µ BASELINE (GPT-3.5-turbo):\n",
            "----------------------------------------------------------------------\n",
            "To calculate the percentage of revenue that came from automotive sales, we need to divide the automotive revenue by the total revenue and then multiply by 100 to get the percentage.\n",
            "\n",
            "Automotive revenue = $19.6 billion\n",
            "Total revenue = $23.4 billion\n",
            "\n",
            "Percentage of revenue from automotive sales = (Automotive revenue / Total revenue) * 100\n",
            "Percentage of revenue from automotive sales = ($19.6B / $23.4B) * 100\n",
            "Percentage of revenue from automotive sales = (0.8376) * 100\n",
            "Percentage of revenue from automotive sales â‰ˆ 83.76%\n",
            "\n",
            "Therefore, approximately 83.76% of Tesla's revenue in Q3 2023 came from automotive sales.\n",
            "\n",
            "ğŸŸ¢ FINE-TUNED (GPT-4o):\n",
            "----------------------------------------------------------------------\n",
            "84%\n",
            "\n",
            "======================================================================\n",
            "âœ… Comparison complete!\n"
          ]
        }
      ]
    }
  ]
}